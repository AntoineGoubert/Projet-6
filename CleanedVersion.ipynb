{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from keras.layers import Rescaling\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LayerNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "dim=100\n",
    "\n",
    "def Extract_Images_Test(Folder):\n",
    "    temp=[]\n",
    "    for filename in glob.glob('C:/Users/antoi/Dropbox/PC/Desktop/Images/' + Folder + '/*.jpg'):\n",
    "        img=cv.imread(filename)\n",
    "        temp.append([img,(os.path.basename(Folder)).split('-')[1].replace('_',' ')])\n",
    "    return(pd.DataFrame(temp,columns=['Image','Race']))\n",
    "\n",
    "\n",
    "def Extract_Images(Folder):\n",
    "    temp=[]\n",
    "    for filename in glob.glob('C:/Users/antoi/Dropbox/PC/Desktop/Images/' + Folder + '/*.jpg'):\n",
    "        img=cv.imread(filename)\n",
    "        img=cv.resize(img,dsize=[100, 100])\n",
    "        temp.append([img,(os.path.basename(Folder)).split('-')[1].replace('_',' ')])\n",
    "        temp.append([cv.flip(img,0),(os.path.basename(Folder)).split('-')[1].replace('_',' ')])\n",
    "        temp.append([cv.flip(img,1),(os.path.basename(Folder)).split('-')[1].replace('_',' ')])\n",
    "        temp.append([cv.flip(img,-1),(os.path.basename(Folder)).split('-')[1].replace('_',' ')])\n",
    "    return(pd.DataFrame(temp,columns=['Image','Race']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "Folders=os.listdir('C:/Users/antoi/Dropbox/PC/Desktop/Images')\n",
    "print(len(Folders))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Extending the models to the whole DataFrame**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "num_classes=120"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "data=pd.DataFrame(columns=['Image','Race','RaceId'],dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for Folder in Folders[:num_classes]:\n",
    "    tempdata=Extract_Images(Folder)\n",
    "    data=pd.concat([data,pd.concat([tempdata,pd.DataFrame([i]*(tempdata.shape[0]),columns=['RaceId'])],axis=1)])\n",
    "    if i%20==0:\n",
    "        print(i)\n",
    "    i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82320, 4)\n"
     ]
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "class_names=pd.unique(data['Race'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Image'].apply(lambda x : cv.cvtColor(x,cv.COLOR_BGR2GRAY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Grayscale'].apply(lambda x : cv.convertScaleAbs(cv.equalizeHist(x),alpha=1,beta=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "data_original=data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "for k in range(data['Image'].shape[0]):\n",
    "    gray=data['Grayscale'][k]\n",
    "    img=data['Image'][k]\n",
    "    dst = cv.cornerHarris(gray,2,3,0.04)\n",
    "    img[dst>0.005*dst.max()]=[255,0,0]\n",
    "    if k%10000==0:\n",
    "        print(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Image'].apply(lambda x : cv.cvtColor(x,cv.COLOR_BGR2GRAY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Grayscale'].apply(lambda x : cv.convertScaleAbs(cv.equalizeHist(x),alpha=1,beta=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "data=pd.concat([data,data_original],axis=0)\n",
    "data_original=data.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164640, 6)\n"
     ]
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [108]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m img\u001B[38;5;241m=\u001B[39mdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mImage\u001B[39m\u001B[38;5;124m'\u001B[39m][k]\n\u001B[0;32m      4\u001B[0m sift \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mxfeatures2d\u001B[38;5;241m.\u001B[39mSIFT_create()\n\u001B[1;32m----> 5\u001B[0m kp \u001B[38;5;241m=\u001B[39m \u001B[43msift\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m img\u001B[38;5;241m=\u001B[39mcv\u001B[38;5;241m.\u001B[39mdrawKeypoints(gray,kp,img)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m10000\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(data['Image'].shape[0]):\n",
    "    gray=data['Grayscale'][k]\n",
    "    img=data['Image'][k]\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    kp = sift.detect(img, None)\n",
    "    img=cv.drawKeypoints(gray,kp,img)\n",
    "    if k%10000==0:\n",
    "        print(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Image'].apply(lambda x : cv.cvtColor(x,cv.COLOR_BGR2GRAY))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['Grayscale']=data['Grayscale'].apply(lambda x : cv.convertScaleAbs(cv.equalizeHist(x),alpha=1,beta=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=pd.concat([data,data_original],axis=0)\n",
    "data=data.drop(['level_0','index'],axis=1)\n",
    "data.reset_index(inplace=True)\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre processing of the data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_image,test_image,train_label, test_label = train_test_split(data.loc[:,['Image','Grayscale']],data.loc[:,['Race','RaceId']],stratify=data['RaceId'],test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_counts = pd.DataFrame({\n",
    "    'data': np.array(['train'] * num_classes + ['test'] * num_classes),\n",
    "    'class': np.tile(np.arange(num_classes), 2),\n",
    "    'prop': np.hstack([np.bincount(train_label['RaceId']) / train_label.shape[0],\n",
    "                         np.bincount(test_label['RaceId']) / test_label.shape[0]])\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.barplot(x='class', y='prop', hue='data', data=y_counts, ax=ax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "k=0\n",
    "for i in train_image.index[:25]:\n",
    "    plt.subplot(5,5,k+1)\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_image['Image'][i])\n",
    "    plt.xlabel(train_label['Race'][i])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We normalize the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_images = train_image['Image']\n",
    "test_images = test_image['Image']\n",
    "\n",
    "train_labels=train_label['RaceId']\n",
    "test_labels=test_label['RaceId']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scale of grey is now from 0 to 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###**Creation of a validation set**\n",
    "\n",
    "We further split the train set into a train and validation set. The validation set will be 20% from the original train set, therefore the split will be train/validation of 0.8/0.2.\n",
    "\n",
    "The actual training set will be divided into two groups:\n",
    "* the training set\n",
    "* the validation set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=2020)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_id=train_images.index\n",
    "valid_id=valid_images.index\n",
    "test_id=test_images.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "partition = {'train' : [str(i) for i in train_id], 'validation' : [str(i) for i in valid_id]}\n",
    "\n",
    "labels={}\n",
    "for i in train_id:\n",
    "    labels[str(i)]=train_labels[i]\n",
    "for i in valid_id:\n",
    "    labels[str(i)]=valid_labels[i]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We define a class used not to overflow our GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "epochs=200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=batch_size, dim=input_shape, n_classes=num_classes, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = data['Image'][int(ID)]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_images=np.asarray([i.astype(np.float32) for i in train_images])\n",
    "\n",
    "valid_images=np.asarray([i.astype(np.float32) for i in valid_images])\n",
    "\n",
    "test_images=np.asarray([i.astype(np.float32) for i in test_images])\n",
    "\n",
    "input_shape = train_images.shape[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "test_labels=np.asarray([i.astype(np.float32) for i in test_labels])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_neural_network(MaxPoolNum=3,PoolSizeList=[(3,3),(3,3),(2,2)],ConvList=[2,2,3],KerSizes=[[(3,3),(3,3)],[(3,3),(3,3)],[(3,3),(3,3),(3,3)]],ConvArgList=[[32,32],[64,64],[128,128,256]],optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'],rescaling=True,input_shape=input_shape,dropout_rate=0.75):\n",
    "    model_temp = Sequential()\n",
    "    if rescaling:\n",
    "        model_temp.add(Rescaling(1./255, input_shape=input_shape))\n",
    "    for k in range(MaxPoolNum):\n",
    "        for i in range(ConvList[k]):\n",
    "            if k==0 and i==0:\n",
    "                model_temp.add(Conv2D(ConvArgList[k][i],kernel_size=KerSizes[k][i],activation='relu', input_shape=input_shape))\n",
    "            else:\n",
    "                model_temp.add(Conv2D(ConvArgList[k][i],kernel_size=KerSizes[k][i],activation='relu'))\n",
    "        if k==0:\n",
    "            model_temp.add(LayerNormalization())\n",
    "        model_temp.add(MaxPooling2D(pool_size=PoolSizeList[k]))\n",
    "\n",
    "\n",
    "    model_temp.add(Flatten())\n",
    "    model_temp.add(Dense(num_classes*5, activation='relu'))\n",
    "    model_temp.add(Dropout(dropout_rate))\n",
    "    model_temp.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_temp.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "    return model_temp\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Generators\n",
    "\n",
    "training_generator = DataGenerator(partition['train'], labels)\n",
    "validation_generator = DataGenerator(partition['validation'], labels)\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10,  restore_best_weights=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time0=time.time()\n",
    "wholemodel_cnn = create_neural_network(MaxPoolNum=3,PoolSizeList=[3,2,2],ConvList=[1,1,1],\n",
    "                                        KerSizes=[[10],[8],[6]],ConvArgList=[[16],[32],[32]],\n",
    "                                        optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "                                        rescaling=True,input_shape=input_shape,dropout_rate=0.4)\n",
    "\n",
    "wholemodel_cnn.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the train\n",
    "wholehistory_cnn = wholemodel_cnn.fit(training_generator,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[es])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wholescore_cnn = wholemodel_cnn.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', wholescore_cnn[0])\n",
    "print('Test accuracy:', wholescore_cnn[1])\n",
    "time1=time.time()-time0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time0=time.time()\n",
    "wholemodel_cnn2 = create_neural_network(MaxPoolNum=3,PoolSizeList=[(3,3),(3,3),(3,3)],ConvList=[2,2,1],\n",
    "                                        KerSizes=[[(3,3),(3,3)],[(4,4),(3,3)],[(3,3)]],ConvArgList=[[32,32],[64,64],[256]],\n",
    "                                        optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "                                        rescaling=True,input_shape=input_shape,dropout_rate=0.7)\n",
    "\n",
    "\n",
    "wholemodel_cnn2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the train\n",
    "wholehistory_cnn2 = wholemodel_cnn2.fit(training_generator,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[es])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wholescore_cnn2 = wholemodel_cnn2.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', wholescore_cnn2[0])\n",
    "print('Test accuracy:', wholescore_cnn2[1])\n",
    "time1=time.time()-time0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Re-training du VGG16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import models\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(num_classes*10, activation='relu')\n",
    "dense_layer_2 = layers.Dense(num_classes*5, activation='relu')\n",
    "dropout_layer = layers.Dropout(0.25)\n",
    "prediction_layer = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "\n",
    "wholemodel = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dropout_layer,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "wholemodel.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "wholehistory_model=wholemodel.fit(training_generator,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=validation_generator,\n",
    "                            callbacks=[es])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wholescore_model = wholemodel.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', wholescore_model[0])\n",
    "print('Test accuracy:', wholescore_model[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_set= (valid_images,valid_labels)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loss\n",
    "print(\"1st CNN MODEL: \")\n",
    "history_loss(wholehistory_cnn)\n",
    "print(\"2nd CNN MODEL:\")\n",
    "history_loss(wholehistory_cnn2)\n",
    "print(\"VGG16 MODEL: \")\n",
    "history_loss(wholehistory_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"1st CNN MODEL: \")\n",
    "history_accuracy(wholehistory_cnn)\n",
    "print(\"2nd CNN MODEL:\")\n",
    "history_accuracy(wholehistory_cnn2)\n",
    "print(\"VGG16 MODEL: \")\n",
    "history_accuracy(wholehistory_model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "CNN = ['CNN 2', 'VGG16']\n",
    "Test_Accuracy = [wholescore_cnn2[1],wholescore_model[1]]\n",
    "ax.bar(CNN,Test_Accuracy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_prediction_images(prediction):\n",
    "    num_rows = 5\n",
    "    num_cols = 3\n",
    "    num_images = num_rows*num_cols\n",
    "    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "    for i in range (num_images):\n",
    "        plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "        plot_image(i, prediction[i], test_labels, test_images)\n",
    "        plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "        plot_value_array(i, prediction[i], test_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_neural_network(MaxPoolNum=3,PoolSizeList=[(3,3),(3,3),(2,2)],ConvList=[2,2,3],KerSizes=[[(3,3),(3,3)],[(3,3),(3,3)],[(3,3),(3,3),(3,3)]],ConvArgList=[[32,32],[64,64],[128,128,256]],optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'],rescaling=True,input_shape=input_shape,dropout_rate=0.75):\n",
    "    model_temp = Sequential()\n",
    "    if rescaling:\n",
    "        model_temp.add(Rescaling(1./255, input_shape=input_shape))\n",
    "    for k in range(MaxPoolNum):\n",
    "        for i in range(ConvList[k]):\n",
    "            if k==0 and i==0:\n",
    "                model_temp.add(Conv2D(ConvArgList[k][i],kernel_size=KerSizes[k][i],activation='relu', input_shape=input_shape))\n",
    "            else:\n",
    "                model_temp.add(Conv2D(ConvArgList[k][i],kernel_size=KerSizes[k][i],activation='relu'))\n",
    "        if k==0:\n",
    "            model_temp.add(LayerNormalization())\n",
    "        model_temp.add(MaxPooling2D(pool_size=PoolSizeList[k]))\n",
    "\n",
    "\n",
    "    model_temp.add(Flatten())\n",
    "    model_temp.add(Dense(num_classes*5, activation='relu'))\n",
    "    model_temp.add(Dropout(dropout_rate))\n",
    "    model_temp.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_temp.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
    "    return model_temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}